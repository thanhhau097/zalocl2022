{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "subprocess.run(f\"cd ./hoanganh && python predict.py --data_folder {data_folder}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(f\"cd ./hoanganh-mergetokens && python predict.py --data_folder {data_folder}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble\n",
    "submission_folders = []\n",
    "for folder in os.listdir(\"./hoanganh/data/\"):\n",
    "    if os.path.isdir(\"./hoanganh/data/\" + folder):\n",
    "        submission_folders.append(\"./hoanganh/data/\" + folder)\n",
    "        \n",
    "for folder in os.listdir(\"./hoanganh-mergetokens/data/\"):\n",
    "    if os.path.isdir(\"./hoanganh-mergetokens/data/\" + folder):\n",
    "        submission_folders.append(\"./hoanganh-mergetokens/data/\" + folder)\n",
    "\n",
    "ensemble_submission_folder = \"ensembled/\"\n",
    "os.makedirs(ensemble_submission_folder, exist_ok=True)\n",
    "vote_lim = 2\n",
    "\n",
    "for file in os.listdir(submission_folders[0]):\n",
    "    datas = []\n",
    "    for i in range(len(submission_folders)):\n",
    "        with open(os.path.join(submission_folders[i], file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        datas.append(data)\n",
    "\n",
    "    words_folds = []\n",
    "    for i in range(len(submission_folders)):\n",
    "        words = []\n",
    "        for segment in datas[i]:\n",
    "            words += segment[\"l\"]\n",
    "        words_folds.append(words)\n",
    "\n",
    "\n",
    "    for i in range(len(words_folds[0])):\n",
    "        words_folds[0][i][\"s\"] = sorted([words_folds[k][i][\"s\"] for k in range(len(submission_folders))])[vote_lim]\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        words_folds[0][i][\"e\"] = sorted([words_folds[k][i][\"e\"] for k in range(len(submission_folders))])[::-1][vote_lim]\n",
    "\n",
    "\n",
    "    with open(os.path.join(ensemble_submission_folder, file), \"w\") as f:\n",
    "        json.dump(datas[0], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune\n",
    "\n",
    "submission_folder = ensemble_submission_folder\n",
    "tune_submission_folder = \"./submissions/\"\n",
    "# tune_submission_folder = \"/result\"\n",
    "os.makedirs(tune_submission_folder, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(submission_folder):\n",
    "    with open(os.path.join(submission_folder, file), \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    words = []\n",
    "\n",
    "    for segment in data:\n",
    "        words += segment[\"l\"]\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        words[i][\"s\"] = words[i][\"s\"] - 20 if words[i][\"s\"] - 20 > 0 else 0\n",
    "        words[i][\"e\"] = words[i][\"e\"] + 10\n",
    "\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i + 1][\"s\"] > words[i][\"e\"]:\n",
    "            words[i][\"e\"] = words[i+1][\"s\"] if words[i+1][\"s\"] - words[i][\"e\"] < 500 else words[i][\"e\"] + 500\n",
    "\n",
    "    with open(os.path.join(tune_submission_folder, file), \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum time and save to csv\n",
    "\n",
    "with open(\"./hoanganh/data/time_submission.json\", \"r\") as f:\n",
    "    time_dict_1 = json.load(f)\n",
    "with open(\"./hoanganh-mergetokens/data/time_submission.json\", \"r\") as f:\n",
    "    time_dict_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in time_dict_1.items():\n",
    "    time_dict_1[k] = v + time_dict_2[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"fname\": list(time_dict_1.keys()), \"time\": list(time_dict_1.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"time_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -qqr jupyter_submissions.zip submissions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
